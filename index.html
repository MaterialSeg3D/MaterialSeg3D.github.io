<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets">
  <meta name="keywords" content="Mesh, Computer Vision, Rendering">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--   <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
   <style>
        .small-image {
            width: 70%; /* 设置图片宽度为50% */
            display: block;
            margin: 0 auto; /* 让图片水平居中显示 */
        }
    </style>



  
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
<!--   <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

<!--       <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Zeyu Li</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Ruitong Gan</a><sup>2,4,*</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Chuanchen Luo</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Yuxi Wang</a><sup>3,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Jiaheng Liu</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Ziwei Zhu</a><sup>7</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Man Zhang</a><sup>1,✉</sup>
            </span>
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Qing Li</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Xucheng Yin</a><sup>7</sup>
            </span>
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Zhaoxiang Zhang</a><sup>3,4,5,✉</sup>
            </span>
            <span class="author-block">
              <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io">Junran Peng</a><sup>3</sup>
            </span>
          </div>
          <div class="is-size-8 publication-authors">
              <span class="author-block">
              (*Equal contribution   ✉Corresponding author)
            </span>
          </div>  
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Beijing University of Posts and Telecommunications,</span>
            <span class="author-block"><sup>2</sup>Hong Kong Polytechnic University,</span>
            <span class="author-block"><sup>3</sup>Institute of Automation, Chinese Academy of Sciences,</span>
            <span class="author-block"><sup>4</sup>CAIR, HKISI-CAS,</span>
            <span class="author-block"><sup>5</sup>University of Chinese Academy of Sciences,</span>
            <span class="author-block"><sup>6</sup>Beijing University of Aeronautics and Astronautics,</span>
            <span class="author-block"><sup>7</sup>University of Science and Technology Beijing</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2404.13923"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.13923"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Gradio Demo Link. -->
              <span class="link-block">
                <a href="https://a8599b14f1a4346890.gradio.live"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/PROPHETE-pro/MaterialSeg3D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/MaterialSeg3D/MaterialSeg3D.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->
<!-- 

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


  
<!--   <img class="small-image" src="./static/images/image1.png" alt="PDF Page 1"> -->
  <video autoplay loop playsinline muted height="100%">
              <source src="./static/videos/all.mp4"
                      type="video/mp4">
  </video>
  <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
  <div class="content has-text-justified"> 
  <p>Renderings of processed assets with material information under different lighting conditions. 
Given a raw asset, our MaterialSeg3D can actively predict and refine dense explicit surface material based on 2D priors. 
Equipped with material definitions, 3D assets support physically based rendering, leading to photorealistic visual effects.
  </p>
<!--   <p>
(a) Renderings of raw 3D assets that only have albedo information. 
(b) Renderings of processed assets with material information under different lighting conditions. 
Given a raw asset, our MaterialSeg3D can actively predict and refine dense explicit surface material based on 2D priors. 
Equipped with material definitions, 3D assets support physically based rendering, leading to photorealistic visual effects.
  </p> -->
</div>
    </div>
        </div>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Driven by powerful image diffusion models, recent research has achieved the automatic creation of 3D objects from textual or visual guidance.
By performing score distillation sampling (SDS) iteratively across different views, these methods succeed in lifting 2D generative prior to the 3D space.
However, such a 2D generative image prior bakes the effect of illumination and shadow into the texture.
As a result, material maps optimized by SDS inevitably involve spurious correlated components.
The absence of precise material definition makes it infeasible to relight the generated assets reasonably in novel scenes, which limits their application in downstream scenarios.
In contrast, humans can effortlessly circumvent this ambiguity by deducing the material of the object from its appearance and semantics.
Motivated by this insight, we propose MaterialSeg3D, a 3D asset material generation framework to infer underlying material from the 2D semantic prior.
Based on such a prior model, we devise a mechanism to parse material in 3D space.
We maintain a UV stack, each map of which is unprojected from a specific viewpoint.
After traversing all viewpoints, we fuse the stack through a weighted voting scheme and then employ region unification to ensure the coherence of the object parts. 
To fuel the learning of semantics prior, we collect a material dataset, named Materialized Individual Objects (MIO), which features abundant images, diverse categories, and accurate annotations.
Extensive quantitative and qualitative experiments demonstrate the effectiveness of our method.
          </p>
        </div>
      </div>
</div>
    </div>
    <!--/ Abstract. -->

<!--     <img class="small-image" src="./static/images/pipeline.png" alt="PDF Page 2"> -->
 

<!--  <object data="./static/images/Method.pdf" type="application/pdf" width="100%" height="600px">
  <p>您的浏览器不支持嵌入 PDF 文件，请下载文件查看：<a href="./static/images/compare.pdf">下载 PDF 文件</a>.</p>
</object> -->

  <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
   <img src="./static/images/Method.svg" alt="Comparison">
  <div class="content has-text-justified">
    <p>
<strong>Overall framework of MaterialSeg3D workflow.</strong> The material segmentation model is trained on MIO beforehand. Multi-view renderings are first generated 
    with pre-defined and randomly selected camera angles and are further inferenced by the material segmentation model and attached to a stacked temporary UV map. Weighted 
    voting and region unification are further applied to generate the final material UV.
    </p>
    </div>
    </div>
    </div>

    <!--/ Paper video. -->
  </div>
</section>
  
<section class="section">
 
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video Introduction</h2>
        <div class="publication-video">
        <video id="demo-all" controls playsinline height="100%">
        <source src="./static/videos/demo-all.mp4"
                      type="video/mp4">
            </video>
<!--           <iframe src="./static/videos/demo-all"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
        </div>
      </div>
    </div>


  

  
<section class="section">

   <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Visualizing Performances</h2>
          <img src="./static/images/compare.svg">
<!-- <object data="./static/images/compare.pdf" type="application/pdf" width="100%" height="600px">
  <p>抱歉，你的浏览器不支持嵌入 PDF 文件，请点击<a href="./static/images/compare.pdf">这里</a>下载 PDF 文件。</p>
</object> -->



          <div class="container is-max-desktop">
          
        <div class="content has-text-justified">
 <p>To evaluate the effectiveness of the proposed material generation method, we compare previous approaches from the following three aspects: single-image-to-3D generation methods, texture generation methods, and public 3D assets. The corresponding results are shown in Figer. Considering single image-to-3D generation methods, we compare state-of-the-art Wonder3D, TripoSR, and OpenLRM in this section. Specifically, given a reference view as input, Wonder3D, TripoSR, and OpenLRM generate a 3D object with referenced texture. We can observe that the provided MaterialSeg3D significantly outperforms the previous work owing to the adoption of well-defined 3D mesh and Albedo information. For a fair comparison, we modify existing texture generation methods like Fantasia3D, Text2Tex, and online functions provided by <i>Meshy</i> (<a href="https://app.meshy.ai/">https://app.meshy.ai/</a>) for evaluation. Given a well-defined geometry mesh, previous work provides texturing results according to the text prompt as shown in Figer. The results demonstrate our method provides much more realistic renderings under different lighting conditions. Note that for Fantasia3D, we only adopt its texture generation (Appearance Modeling) stage during comparison. Moreover, we also provide material generation results for 3D assets obtained from public websites, exemplified as <i>tripo3d</i> (<a href="https://www.tripo3d.ai/app/">https://www.tripo3d.ai/app/</a>) and <i>turbosquid</i> (<a href="https://www.turbosquid.com/">https://www.turbosquid.com/</a>). From the results in Figer, we can observe the proposed MaterialSeg3D can generate precise PBR material information while significantly improving the overall quality of the assets.</p>
         </div>
      </div>
   </div>
       </div>
</section>
    
    <!--/ Abstract. -->

</section>

<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
 -->
<!--       Visual Effects. -->
<!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Example result 1</h2>
<!--           <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p> -->
<!--           <video id="dollyzoom" autoplay loop playsinline muted height="100%">
            <source src="./static/videos/1.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> --> 
      <!--/ Visual Effects. -->

      <!-- Matting. -->
<!--       <div class="column">
        <h2 class="title is-3">Example result 2</h2>
        <div class="columns is-centered">
          <div class="column content">
<!--             <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p> -->
<!--             <video  autoplay loop playsinline muted height="100%>
              <source src="./static/videos/2.mp4"
                      type="video/mp4"> 
<!--             </video>
          </div>

        </div>
      </div>
    </div> --> 
    <!--/ Matting. -->
<section class="section">
<div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitative Results</h2>
         <img src="./static/images/result.png" alt="PDF Page 2">
          
        <div class="content has-text-justified">

<p>
  Furthermore, we also provide quantitative results comparing our method and existing Image-to-3D methods including Wonder3D, TripoSR, and OpenLRM. We adopt CLIP Similarity, PSNR, and SSIM as the evaluations, and the corresponding results are shown in Table. We choose assets from Objaverse-1.0 dataset as the test sample and randomly select three camera angles as novel views. The ground-truth reference and novel views are captured from assets with ground-truth material information and fixed lighting conditions. Given a well-defined 3D mesh and Albedo, our workflow can provide reliable PBR material, resulting in more realistic rendering visual effects.
</p>
        </div>
        </div>
</div>
    </div>
<!--   <div class="container is-max-desktop content"> -->
    <!--/ Abstract. -->
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{li2024materialseg3d,
  title     = {MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets},
  author    = {Li, Zeyu and Gan, Ruitong and Luo, Chuanchen and Wang, Yuxi and Liu, Jiaheng and Zhang, Ziwei Zhu Man and Li, Qing and Yin, Xucheng and Zhang, Zhaoxiang and Peng, Junran},
  journal   = {arXiv preprint arXiv:2404.13923},
  year      = {2024}
}</code></pre>
<!--  </div> -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified"> -->
 <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
<!--         </div>
</div>
    </div> -->
</section>

<!--     <!-- Animation. -->
<!--     <div class="columns is-centered"> -->
<!--       <div class="column is-full-width"> --> -->
<!--         <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
<!--         <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
<!--         <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->
<!-- 
      </div>
    </div> -->
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
<!--     <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->
<!-- 
  </div>
</section>
 -->

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{li2024materialseg3d,
  title     = {MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets},
  author    = {Li, Zeyu and Gan, Ruitong and Luo, Chuanchen and Wang, Yuxi and Liu, Jiaheng and Zhang, Ziwei Zhu Man and Li, Qing and Yin, Xucheng and Zhang, Zhaoxiang and Peng, Junran},
  journal   = {arXiv preprint arXiv:2404.13923},
  year      = {2024}
}</code></pre>
      </div> 
</section>
 -->
<!-- <footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2404.13923">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/PROPHETE-pro/MaterialSeg3D_" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->
  
</body>
</html>
